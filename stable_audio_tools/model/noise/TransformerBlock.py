import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
from .utils import LayerNorm
from .Attn import Attention
from .FeedForaward import FeedForward

class TransformerBlock(nn.Module):
    def __init__(
            self,
            dim,
            dim_heads = 64,
            cross_attend = False,
            dim_context = None,
            global_cond_dim = None,
            zero_init_branch_outputs = True,
            remove_norms = False,
            attn_kwargs = {},
            ff_kwargs = {},
            norm_kwargs = {}
    ):
        super().__init__()
        self.dim = dim
        self.dim_heads = dim_heads
        self.cross_attend = cross_attend
        self.dim_context = dim_context

        self.pre_norm = LayerNorm(dim, **norm_kwargs) if not remove_norms else nn.Identity()
        self.self_attn = Attention(
            dim,
            dim_heads = dim_heads,
            zero_init_output=zero_init_branch_outputs,
            **attn_kwargs
        )

        if cross_attend:
            self.cross_attend_norm = LayerNorm(dim, **norm_kwargs) if not remove_norms else nn.Identity()
            self.cross_attn = Attention(
                dim,
                dim_heads = dim_heads,
                dim_context=dim_context,
                zero_init_output=zero_init_branch_outputs,
                **attn_kwargs
            )
        
        self.ff_norm = LayerNorm(dim, **norm_kwargs) if not remove_norms else nn.Identity()
        self.ff = FeedForward(dim, zero_init_output=zero_init_branch_outputs, **ff_kwargs)

        # self.global_cond_dim = global_cond_dim
        # if global_cond_dim is not None:
        #     self.to_scale_shift_gate = nn.Sequential(
        #         nn.SiLU(),
        #         nn.Linear(global_cond_dim, dim * 6, bias=False)
        #     )
        #     nn.init.zeros_(self.to_scale_shift_gate[1].weight)

    def forward(
        self,
        x,
        context = None,
        global_cond=None,
        mask = None,
        context_mask = None,
        rotary_pos_emb = None
    ):
        # below is utilize global_cond with adaLN
        if self.global_cond_dim is not None and self.global_cond_dim > 0 and global_cond is not None:
            # scale_self, shift_self, gate_self, scale_ff, shift_ff, gate_ff = self.to_scale_shift_gate(global_cond).unsqueeze(1).chunk(6, dim = -1)

            # # self-attention with adaLN
            # residual = x
            # x = self.pre_norm(x)
            # x = x * (1 + scale_self) + shift_self
            # x = self.self_attn(x, mask = mask, rotary_pos_emb = rotary_pos_emb)
            # x = x * torch.sigmoid(1 - gate_self)
            # x = x + residual

            # # cross-attention
            # if context is not None:
            #     x = x + self.cross_attn(self.cross_attend_norm(x), context = context, context_mask = context_mask)

            # # feedforward with adaLN
            # residual = x
            # x = self.ff_norm(x)
            # x = x * (1 + scale_ff) + shift_ff
            # x = self.ff(x)
            # x = x * torch.sigmoid(1 - gate_ff)
            # x = x + residual
            pass

        else:
            x = x + self.self_attn(self.pre_norm(x), mask = mask, rotary_pos_emb = rotary_pos_emb)
            if context is not None:
                x = x + self.cross_attn(self.cross_attend_norm(x), context = context, context_mask = context_mask)
            x = x + self.ff(self.ff_norm(x))

        return x